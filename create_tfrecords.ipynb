{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e47d0c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'  #'last', 'last_expr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c07fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "import seqio\n",
    "from t5.data import preprocessors as t5_preprocessors\n",
    "from praxis import base_input, base_hyperparams\n",
    "from paxml import seqio_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f38c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'text': [[1, 2, 3], [2, 4]], 'label': [0, 1]}\n",
    "ds = Dataset.from_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6355e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"rotten_tomatoes\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d4b2e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n",
      "{'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .', 'label': 1}\n",
      "{'text': 'effective but too-tepid biopic', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(dataset):\n",
    "    if i == 3: break\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7b383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = '/nas/xd/.cache/torch/transformers/'\n",
    "model_name = 'EleutherAI/gpt-j-6B'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_function(examples): return tokenizer(example['text'])\n",
    "tokenized_dataset = dataset.map(lambda example: tokenizer(example['text']),\n",
    "    batched=True, num_proc=None, remove_columns=dataset.column_names)  # run_clm_flax.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns('attention_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa902afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1169, 3881, 318, 23985, 284, 307, 262, 2310, 301, 4289, 338, 649, 366, 369, 272, 366, 290, 326, 339, 338, 1016, 284, 787, 257, 22870, 772, 3744, 621, 610, 77, 727, 5513, 5767, 89, 44028, 837, 474, 11025, 12, 565, 3885, 5719, 1801, 1326, 393, 2876, 574, 384, 13528, 764]}\n",
      "{'input_ids': [1169, 17177, 3481, 15962, 24659, 286, 366, 262, 15876, 286, 262, 13917, 366, 26298, 318, 523, 3236, 326, 257, 5721, 286, 2456, 2314, 22668, 6901, 763, 12, 16002, 14, 35248, 279, 2357, 14509, 1559, 338, 9902, 5761, 286, 474, 764, 374, 764, 374, 764, 284, 75, 74, 2013, 338, 3504, 12, 16442, 764]}\n",
      "{'input_ids': [16803, 475, 1165, 12, 83, 538, 312, 3182, 16603]}\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(tokenized_dataset):\n",
    "    if i == 3: break\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef39a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mesh-transformer-jax, https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
    "def _int64_feature(value): return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def write_tfrecords(dataset, fp):\n",
    "    with tf.io.TFRecordWriter(fp) as writer:\n",
    "        for example in dataset:\n",
    "            feature = {\"input_ids\": _int64_feature(example['input_ids'])}\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b48f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = f'{dataset_name}_train_{len(tokenized_dataset)}.tfrecords'\n",
    "write_tfrecords(tokenized_dataset, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6b4f9a",
   "metadata": {},
   "source": [
    "## load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7e26969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_desc = {\"input_ids\": tf.io.VarLenFeature(tf.int64)}\n",
    "def _parse_function(example_proto): # https://zhuanlan.zhihu.com/p/552951305\n",
    "    return tf.io.parse_single_example(example_proto, feature_desc)\n",
    "#     for name in list(example.keys()):\n",
    "#         t = example[name]\n",
    "#         if t.dtype == tf.int64: t = tf.cast(t, dtype=tf.int32)\n",
    "#         example[name] = tf.sparse.to_dense(t, default_value=0)\n",
    "        # example[name] = tf.sparse.to_dense(tf.sparse.reorder(t)) # mesh-transformer-jax\n",
    "#     return example\n",
    "\n",
    "def shard(data, batch_size=None):\n",
    "    return jax.tree_map(lambda x: x.numpy().reshape(batch_size + x.shape[1:]), data)  # mtj\n",
    "    \n",
    "def prefetch(dataset, n_prefetch=None):\n",
    "    # Taken from: https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py\n",
    "    ds_iter = iter(dataset)\n",
    "    ds_iter = map(lambda x: jax.tree_map(lambda t: np.asarray(memoryview(t)), x), ds_iter)\n",
    "    if n_prefetch: ds_iter = flax.jax_utils.prefetch_to_device(ds_iter, n_prefetch)\n",
    "    return ds_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e7d9f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = '/nas/xd/projects/mesh-transformer-jax/rotten_tomatoes_train_8530.tfrecords'\n",
    "fp = 'gs://jax_projects/data/rotten_tomatoes_train_8530.tfrecords'  # v3-8\n",
    "fp = 'gs://llm_projects/data/rotten_tomatoes_train_8530.tfrecords'  # pod\n",
    "ds = tf.data.TFRecordDataset(fp)\n",
    "# ds = ds.shuffle(buffer_size=min(1000, len(sequences))) # flaxmodels, https://zhuanlan.zhihu.com/p/552951305\n",
    "ds = ds.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds = ds.map(lambda d: {k: tf.cast(tf.sparse.to_dense(v, default_value=0), dtype=tf.int32) for k, v in d.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8636f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(50,), dtype=int32, numpy=\n",
      "array([ 1169,  3881,   318, 23985,   284,   307,   262,  2310,   301,\n",
      "        4289,   338,   649,   366,   369,   272,   366,   290,   326,\n",
      "         339,   338,  1016,   284,   787,   257, 22870,   772,  3744,\n",
      "         621,   610,    77,   727,  5513,  5767,    89, 44028,   837,\n",
      "         474, 11025,    12,   565,  3885,  5719,  1801,  1326,   393,\n",
      "        2876,   574,   384, 13528,   764], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(53,), dtype=int32, numpy=\n",
      "array([ 1169, 17177,  3481, 15962, 24659,   286,   366,   262, 15876,\n",
      "         286,   262, 13917,   366, 26298,   318,   523,  3236,   326,\n",
      "         257,  5721,   286,  2456,  2314, 22668,  6901,   763,    12,\n",
      "       16002,    14, 35248,   279,  2357, 14509,  1559,   338,  9902,\n",
      "        5761,   286,   474,   764,   374,   764,   374,   764,   284,\n",
      "          75,    74,  2013,   338,  3504,    12, 16442,   764],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(9,), dtype=int32, numpy=\n",
      "array([16803,   475,  1165,    12,    83,   538,   312,  3182, 16603],\n",
      "      dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(ds):\n",
    "    if i == 3: break\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "57454dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'input_ids': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}\n",
    "output_features = {k.replace('input_ids', 'targets'): seqio.Feature(vocabulary=None, dtype=tspec.dtype, rank=tspec.shape.rank)\n",
    "                   for k, tspec in ds.element_spec.items()}\n",
    "max_len = 80  # max(len(s) for s in sequences) == 78a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e1872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "49b7b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = 16\n",
    "# train_mbs_per_replica = 2 # train_micro_batch_size_per_gpu in deepspeed\n",
    "tpu_size = 8  # jax.num_devices()\n",
    "cores_per_replica = 4  # mp=4, dp=8/4=2\n",
    "per_replica_batch = 1  # train_mbs_per_replica in run_clm_mp_xd.py, train_micro_batch_size_per_gpu in deepspeed\n",
    "train_batch_size = (gradient_accumulation_steps, per_replica_batch * tpu_size // cores_per_replica)\n",
    "\n",
    "# ds = ds.apply(tf.data.experimental.dense_to_ragged_batch(np.prod(self.bs), drop_remainder=True)) # mtj\n",
    "ds = ds.padded_batch(batch_size=np.prod(train_batch_size), padded_shapes={'input_ids': [max_len]},\n",
    "                     padding_values={'input_ids': 0}, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ff03981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(32, 80), dtype=int32, numpy=\n",
      "array([[ 1169,  3881,   318, ...,     0,     0,     0],\n",
      "       [ 1169, 17177,  3481, ...,     0,     0,     0],\n",
      "       [16803,   475,  1165, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  270,  5419,   326, ...,     0,     0,     0],\n",
      "       [ 5162,  4741,  2308, ...,     0,     0,     0],\n",
      "       [   64,  4958,   913, ...,     0,     0,     0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for batch in ds: print(batch); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c2cc8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.prefetch(10)  # mesh-transformer-jax\n",
    "# ds = ds.repeat()  # gpt-neo/inputs.py\n",
    "# map shard directly over ds won't work, getting AttributeError: 'Tensor' object has no attribute 'numpy'\n",
    "# because inside tf.function?, see e.g.:\n",
    "# 1) https://stackoverflow.com/questions/34097281/convert-a-tensor-to-numpy-array-in-tensorflow\n",
    "# 2) https://github.com/tensorflow/tensorflow/issues/27519\n",
    "# ds = ds.map(partial(shard, batch_size=train_batch_size), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# matthias-wright/flaxmodels/training/stylegan2/data_pipeline.py\n",
    "ds_iter = iter(ds)\n",
    "ds_iter = map(lambda x: shard(x, batch_size=train_batch_size), ds_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4ffdb061",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': array([[[ 1169,  3881,   318, ...,     0,     0,     0],\n",
      "        [ 1169, 17177,  3481, ...,     0,     0,     0]],\n",
      "\n",
      "       [[16803,   475,  1165, ...,     0,     0,     0],\n",
      "        [  361,   345,  3360, ...,     0,     0,     0]],\n",
      "\n",
      "       [[24677,  3212,   355, ...,     0,     0,     0],\n",
      "        [ 1169,  2646,  3769, ...,     0,     0,     0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 2777,  1304,   805, ...,     0,     0,     0],\n",
      "        [  272,  7306,  2569, ...,     0,     0,     0]],\n",
      "\n",
      "       [[  265,   546,  6957, ...,     0,     0,     0],\n",
      "        [  270,  5419,   326, ...,     0,     0,     0]],\n",
      "\n",
      "       [[ 5162,  4741,  2308, ...,     0,     0,     0],\n",
      "        [   64,  4958,   913, ...,     0,     0,     0]]], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "for batch in ds_iter: print(batch); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a9cf7d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2, 80)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@seqio.map_over_dataset\n",
    "def convert_datatype(ex): return {k: tf.cast(tf.sparse.to_dense(v, default_value=0), dtype=tf.int32) for k, v in ex.items()}\n",
    "\n",
    "task_feature_lengths = {'targets': max_len}\n",
    "\n",
    "task_name = \"rotten_tomatoes_train_8530\"\n",
    "# seqio.TaskRegistry.add(task_name, \n",
    "task = seqio.Task(task_name, \n",
    "    seqio.TFExampleDataSource(split_to_filepattern={'train': fp}, feature_description=feature_desc),\n",
    "    preprocessors=[\n",
    "        convert_datatype,\n",
    "        # from paxml.c4 TaskRegistry.add_versioned_tfds_task('c4_lm_v301_gpt', ...\n",
    "        partial(t5_preprocessors.rekey, key_map={'targets': 'input_ids'}),\n",
    "    ],\n",
    "    output_features=output_features,\n",
    ")\n",
    "\n",
    "# dataset = seqio.get_mixture_or_task(task_name).get_dataset(sequence_length=task_feature_lengths, split=\"train\")\n",
    "dataset = task.get_dataset(sequence_length=task_feature_lengths, split=\"train\", shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039563d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = seqio_input.SeqIOInput.HParams(\n",
    "    mixture_name='super_glue_wsc_v102_simple_train',\n",
    "    split_name='train',\n",
    "    task_feature_lengths={'targets': 1280},\n",
    "    feature_converter=seqio_input.LanguageModelFeatures(pack=True),\n",
    "    is_training=True,\n",
    "    use_cached=False,\n",
    "    input_random_seed=123,\n",
    "    batch_size=4)\n",
    "\n",
    "is_training = True\n",
    "p = seqio_input.SeqIOInput.HParams(\n",
    "    name='rtTrain' if is_training else 'rtValid',\n",
    "#     mixture_name= \"rotten_tomatoes_train_8530\" if is_training else \"rotten_tomatoes_dev_100\",\n",
    "    mixture_or_task=task,\n",
    "    split_name='train' if is_training else 'valid',\n",
    "    task_feature_lengths={'targets': 128},\n",
    "    use_cached=False,\n",
    "    repeat=True if is_training else False,\n",
    "    feature_converter=seqio_input.LanguageModelFeatures(\n",
    "        pack=True if is_training else False,\n",
    "#             use_custom_packing_ops=False,\n",
    "            bos_id=0,\n",
    "#             reverse_bos_padding=True, eos_id=0,\n",
    "    ),\n",
    "    is_training=is_training,\n",
    "    input_random_seed=(42 if is_training else 4321),\n",
    "    batch_size=2,  # TODO\n",
    "    drop_remainder=True if is_training else False,\n",
    "#         num_batches_to_skip=self.TRAINING_NUM_BATCHES_TO_SKIP,\n",
    "    num_infeed_hosts=1,  # TODO\n",
    "    reset_for_eval=False if is_training else True,\n",
    "    annotate_padding_fields=True,\n",
    "    shuffle=False,  # debug\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cbfa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = p.clone().set(infeed_host_index=0, num_infeed_hosts=2)\n",
    "p1 = p.clone().set(infeed_host_index=1, num_infeed_hosts=2)\n",
    "inp0 = base_hyperparams.instantiate(p0)\n",
    "inp1 = base_hyperparams.instantiate(p1)\n",
    "\n",
    "batch = inp0.get_next()\n",
    "for k, v in batch.FlattenItems():\n",
    "  print(k, v.shape, v.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd18e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow==2.9.2\n",
    "pip install tensorflow-text==2.9.0\n",
    "\n",
    "pip install ipython\n",
    "\n",
    "git clone https://github.com/xiaoda99/seqio.git\n",
    "git clone https://github.com/xiaoda99/praxis.git\n",
    "git clone https://github.com/xiaoda99/paxml.git\n",
    "# from paxml: orbax-checkpoint 0.2.3 has requirement jax>=0.4.9\n",
    "git clone https://github.com/xiaoda99/praxis.git projects/praxis; cd projects/praxis; git checkout xd/dev; /home/lishengping/miniconda3/bin/pip install -e .\n",
    "git clone https://github.com/xiaoda99/paxml.git projects/paxml; cd projects/paxml; git checkout xd/dev; /home/lishengping/miniconda3/bin/pip install -e .\n",
    "\n",
    "gcloud compute tpus tpu-vm scp /nas/xd/projects/paxml/paxml/tasks/lm/model_params.py llm-jax-v3-32:/home/lishengping/projects/paxml/paxml/tasks/lm/ --zone=us-east1-d  --project llm-tpu --worker all\n",
    "gcloud compute tpus tpu-vm scp /nas/xd/projects/paxml/paxml/tasks/lm/params/c4.py llm-jax-v3-32:/home/lishengping/projects/paxml/paxml/tasks/lm/params/ --zone=us-east1-d  --project llm-tpu --worker all\n",
    "gcloud compute tpus tpu-vm ssh llm-jax-v3-32 --zone=us-east1-d  --project llm-tpu --worker all --command=\"/home/lishengping/miniconda3/bin/python /home/lishengping/projects/paxml/paxml/main.py --exp=tasks.lm.params.c4.C4Spmd2BAdam32Replicas --job_log_dir=gs://llm_projects/log/C4Spmd2BAdam32Replicas 2>&1 | tee train.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ea96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_shape = (16, 2)\n",
    "devices = np.asarray(jax.devices()).reshape(*mesh_shape)\n",
    "mesh = Mesh(devices, ('x', 'y'))\n",
    "\n",
    "if jax.process_index() == 0:\n",
    " input_data = np.arange(0,16).reshape(8,2)\n",
    "elif jax.process_index() == 1:\n",
    " input_data = np.arange(16,32).reshape(8,2)\n",
    "elif jax.process_index() == 2:\n",
    " input_data = np.arange(32,48).reshape(8,2)\n",
    "else:\n",
    " input_data = np.arange(48,64).reshape(8,2)\n",
    "rank = jax.process_index()\n",
    "input_data = np.arange(rank * 16, (rank + 1) * 16).reshape(8,2); input_data\n",
    "f = pjit(lambda x: x, in_axis_resources=P(('x', 'y'),), out_axis_resources=P('x', 'y'))\n",
    "with mesh: data = f(input_data)\n",
    "with mesh:\n",
    "    multihost_utils.sync_global_devices('aaa')\n",
    "    data = f(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a2b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "from jax import pmap\n",
    "from jax.experimental.maps import Mesh\n",
    "from jax.experimental.pjit import pjit\n",
    "from jax.experimental import PartitionSpec as P\n",
    "mesh_shape = (16, 2)\n",
    "devices = np.asarray(jax.devices()).reshape(*mesh_shape)\n",
    "mesh = Mesh(devices, ('x', 'y'))\n",
    "rank = jax.process_index()\n",
    "input_data = np.arange(rank * 16, (rank + 1) * 16).reshape(8,2)\n",
    "rank\n",
    "rank = jax.process_index()\n",
    "input_data = np.arange(rank * 16, (rank + 1) * 16).reshape(8,2); input_data\n",
    "f = pjit(lambda x: x, in_axis_resources=P(('x', 'y'),), out_axis_resources=P('x', 'y'))\n",
    "with mesh: data = f(input_data)\n",
    "with Mesh(mesh.devices, mesh.axis_names): data = f(input_data)\n",
    "from jax.experimental import multihost_utils\n",
    "multihost_utils.sync_global_devices('aaa')\n",
    "with mesh:\n",
    "    multihost_utils.sync_global_devices('aaa')\n",
    "    data = f(input_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
