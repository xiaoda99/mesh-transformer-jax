{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e4ddcdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b24b5bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4104d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d659f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'text': [[1, 2, 3], [2, 4]], 'label': [0, 1]}\n",
    "ds = Dataset.from_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d147145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"rotten_tomatoes\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220aae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = '/nas/xd/.cache/torch/transformers/'\n",
    "model_name = 'EleutherAI/gpt-j-6B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples): return tokenizer(examples['text'])\n",
    "tokenized_dataset = dataset.map(lambda examples: tokenizer(examples['text']),\n",
    "    batched=True, num_proc=None, remove_columns=dataset.column_names)  # run_clm_flax.py\n",
    "tokenized_dataset = tokenized_dataset.remove_columns('attention_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7f24f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mesh-transformer-jax, https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
    "def _int64_feature(value): return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def write_tfrecords(sequences, fp):\n",
    "    with tf.io.TFRecordWriter(fp) as writer:\n",
    "        for seq in sequences:\n",
    "            feature = {\"input_ids\": _int64_feature(seq)}\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            writer.write(example.SerializeToString())\n",
    "            \n",
    "def _parse_function(example_proto): # https://zhuanlan.zhihu.com/p/552951305\n",
    "    feature_desc = {\"input_ids\": tf.io.VarLenFeature(tf.int64)}\n",
    "    example = tf.io.parse_single_example(example_proto, feature_desc)\n",
    "    for name in list(example.keys()):\n",
    "        t = example[name]\n",
    "        if t.dtype == tf.int64: t = tf.cast(t, dtype=tf.int32)\n",
    "        example[name] = tf.sparse.to_dense(t, default_value=0)\n",
    "        # example[name] = tf.sparse.to_dense(tf.sparse.reorder(t)) # mesh-transformer-jax\n",
    "    return example\n",
    "\n",
    "def shard(data, batch_size=None):\n",
    "    return jax.tree_map(lambda x: x.numpy().reshape(batch_size + x.shape[1:]), data)  # mtj\n",
    "    \n",
    "def prefetch(dataset, n_prefetch=None):\n",
    "    # Taken from: https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py\n",
    "    ds_iter = iter(dataset)\n",
    "    ds_iter = map(lambda x: jax.tree_map(lambda t: np.asarray(memoryview(t)), x), ds_iter)\n",
    "    if n_prefetch: ds_iter = flax.jax_utils.prefetch_to_device(ds_iter, n_prefetch)\n",
    "    return ds_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2a6a5bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenized_dataset['input_ids']\n",
    "fp = f'{dataset_name}_train_{len(sequences)}.tfrecords'\n",
    "write_tfrecords(sequences, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1f15cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.TFRecordDataset(fp)\n",
    "# ds = ds.shuffle(buffer_size=min(1000, len(sequences))) # flaxmodels, https://zhuanlan.zhihu.com/p/552951305\n",
    "ds = ds.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "59940281",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = 8\n",
    "train_mbs_per_replica = 2 # train_micro_batch_size_per_gpu in deepspeed\n",
    "mp_size, dp_size = 8, 1\n",
    "train_batch_size = (gradient_accumulation_steps, train_mbs_per_replica * dp_size)\n",
    "max_len = 80  # max(len(s) for s in sequences) == 78\n",
    "# ds = ds.apply(tf.data.experimental.dense_to_ragged_batch(np.prod(self.bs), drop_remainder=True)) # mtj\n",
    "ds = ds.padded_batch(batch_size=np.prod(train_batch_size), padded_shapes={'input_ids': [max_len]},\n",
    "                     padding_values={'input_ids': 0}, drop_remainder=True)\n",
    "ds = ds.prefetch(10)  # mesh-transformer-jax\n",
    "# ds = ds.repeat()  # gpt-neo/inputs.py\n",
    "# map shard directly over ds won't work, getting AttributeError: 'Tensor' object has no attribute 'numpy'\n",
    "# because inside tf.function?, see e.g.:\n",
    "# 1) https://stackoverflow.com/questions/34097281/convert-a-tensor-to-numpy-array-in-tensorflow\n",
    "# 2) https://github.com/tensorflow/tensorflow/issues/27519\n",
    "# ds = ds.map(partial(shard, batch_size=train_batch_size), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# matthias-wright/flaxmodels/training/stylegan2/data_pipeline.py\n",
    "ds_iter = iter(ds)\n",
    "ds_iter = map(lambda x: shard(x, batch_size=train_batch_size), ds_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ca81eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in ds_iter: break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
