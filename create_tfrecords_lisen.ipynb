{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6dc4dbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c0a588d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I saw a man with binoculars.',\n",
       " 'Flying planes can be dangerous.',\n",
       " 'I saw her duck.',\n",
       " \"I can't wait to see you!\",\n",
       " 'The old man and woman were dancing.',\n",
       " 'Police help dog bite victim.',\n",
       " 'Scientists find cure for the common cold.',\n",
       " 'Kids make nutritious snacks.',\n",
       " 'New study shows decline in teen smoking.',\n",
       " 'Large hole appears in High Street.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "sentences= '''\"I saw a man with binoculars.\"\n",
    "\"Flying planes can be dangerous.\"\n",
    "\"I saw her duck.\"\n",
    "\"I can't wait to see you!\"\n",
    "\"The old man and woman were dancing.\"\n",
    "\"Police help dog bite victim.\"\n",
    "\"Scientists find cure for the common cold.\"\n",
    "\"Kids make nutritious snacks.\"\n",
    "\"New study shows decline in teen smoking.\"\n",
    "\"Large hole appears in High Street.\"\n",
    "'''\n",
    "sentences = [s[1: -1] for s in sentences.split('\\n') if s.strip()]\n",
    "sentences\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# 示例句子\n",
    "sentence = \"NLTK is a powerful library for natural language processing.\"\n",
    "\n",
    "# 分词\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# 词性标注\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "# 输出标注结果\n",
    "print(tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecca8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d2569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb49f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e47d0c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'  #'last', 'last_expr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f38c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'text': [[1, 2, 3], [2, 4]], 'label': [0, 1]}\n",
    "ds = Dataset.from_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6355e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"rotten_tomatoes\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d16a1cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 8530\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1bf316e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       "  'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .',\n",
       "  'effective but too-tepid biopic'],\n",
       " 'label': [1, 1, 1]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d4b2e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n",
      "{'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .', 'label': 1}\n",
      "{'text': 'effective but too-tepid biopic', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(dataset):\n",
    "    if i == 3: break\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7b383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = '/nas/xd/.cache/torch/transformers/'\n",
    "model_name = 'EleutherAI/gpt-j-6B'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "29ef5f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [10248, 3329, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Good morning!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_function(examples): return tokenizer(example['text'])\n",
    "tokenized_dataset = dataset.map(lambda example: tokenizer(example['text']),\n",
    "    batched=True, num_proc=None, remove_columns=dataset.column_names)  # run_clm_flax.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ccbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns('attention_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa902afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1169, 3881, 318, 23985, 284, 307, 262, 2310, 301, 4289, 338, 649, 366, 369, 272, 366, 290, 326, 339, 338, 1016, 284, 787, 257, 22870, 772, 3744, 621, 610, 77, 727, 5513, 5767, 89, 44028, 837, 474, 11025, 12, 565, 3885, 5719, 1801, 1326, 393, 2876, 574, 384, 13528, 764]}\n",
      "{'input_ids': [1169, 17177, 3481, 15962, 24659, 286, 366, 262, 15876, 286, 262, 13917, 366, 26298, 318, 523, 3236, 326, 257, 5721, 286, 2456, 2314, 22668, 6901, 763, 12, 16002, 14, 35248, 279, 2357, 14509, 1559, 338, 9902, 5761, 286, 474, 764, 374, 764, 374, 764, 284, 75, 74, 2013, 338, 3504, 12, 16442, 764]}\n",
      "{'input_ids': [16803, 475, 1165, 12, 83, 538, 312, 3182, 16603]}\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(tokenized_dataset):\n",
    "    if i == 3: break\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef39a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mesh-transformer-jax, https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
    "def _int64_feature(value): return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def write_tfrecords(dataset, fp):\n",
    "    with tf.io.TFRecordWriter(fp) as writer:\n",
    "        for example in dataset:\n",
    "            feature = {\"input_ids\": _int64_feature(example['input_ids'])}\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b48f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = f'{dataset_name}_train_{len(tokenized_dataset)}.tfrecords'\n",
    "write_tfrecords(tokenized_dataset, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6b4f9a",
   "metadata": {},
   "source": [
    "## load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7e26969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto): # https://zhuanlan.zhihu.com/p/552951305\n",
    "    feature_desc = {\"input_ids\": tf.io.VarLenFeature(tf.int64)}\n",
    "    example = tf.io.parse_single_example(example_proto, feature_desc)\n",
    "#     for name in list(example.keys()):\n",
    "#         t = example[name]\n",
    "#         if t.dtype == tf.int64: t = tf.cast(t, dtype=tf.int32)\n",
    "#         example[name] = tf.sparse.to_dense(t, default_value=0)\n",
    "        # example[name] = tf.sparse.to_dense(tf.sparse.reorder(t)) # mesh-transformer-jax\n",
    "    return example\n",
    "\n",
    "def shard(data, batch_size=None):\n",
    "    return jax.tree_map(lambda x: x.numpy().reshape(batch_size + x.shape[1:]), data)  # mtj\n",
    "    \n",
    "def prefetch(dataset, n_prefetch=None):\n",
    "    # Taken from: https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py\n",
    "    ds_iter = iter(dataset)\n",
    "    ds_iter = map(lambda x: jax.tree_map(lambda t: np.asarray(memoryview(t)), x), ds_iter)\n",
    "    if n_prefetch: ds_iter = flax.jax_utils.prefetch_to_device(ds_iter, n_prefetch)\n",
    "    return ds_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e7d9f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = '/nas/xd/projects/mesh-transformer-jax/rotten_tomatoes_train_8530.tfrecords'\n",
    "ds = tf.data.TFRecordDataset(fp)\n",
    "# ds = ds.shuffle(buffer_size=min(1000, len(sequences))) # flaxmodels, https://zhuanlan.zhihu.com/p/552951305\n",
    "ds = ds.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bbf2c370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(50,), dtype=int32, numpy=\n",
      "array([ 1169,  3881,   318, 23985,   284,   307,   262,  2310,   301,\n",
      "        4289,   338,   649,   366,   369,   272,   366,   290,   326,\n",
      "         339,   338,  1016,   284,   787,   257, 22870,   772,  3744,\n",
      "         621,   610,    77,   727,  5513,  5767,    89, 44028,   837,\n",
      "         474, 11025,    12,   565,  3885,  5719,  1801,  1326,   393,\n",
      "        2876,   574,   384, 13528,   764], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(53,), dtype=int32, numpy=\n",
      "array([ 1169, 17177,  3481, 15962, 24659,   286,   366,   262, 15876,\n",
      "         286,   262, 13917,   366, 26298,   318,   523,  3236,   326,\n",
      "         257,  5721,   286,  2456,  2314, 22668,  6901,   763,    12,\n",
      "       16002,    14, 35248,   279,  2357, 14509,  1559,   338,  9902,\n",
      "        5761,   286,   474,   764,   374,   764,   374,   764,   284,\n",
      "          75,    74,  2013,   338,  3504,    12, 16442,   764],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(9,), dtype=int32, numpy=\n",
      "array([16803,   475,  1165,    12,    83,   538,   312,  3182, 16603],\n",
      "      dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(ds):\n",
    "    if i == 3: break\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9c6eb161",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(lambda d: {k: tf.cast(tf.sparse.to_dense(v, default_value=0), dtype=tf.int32) for k, v in d.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853bfe7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7344b674",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(22,), dtype=int64, numpy=\n",
       " array([  361,   345,  3360,   588,   284,   467,   284,   262,  6918,\n",
       "          284,   423,  1257,   837,   373, 17914,   318,   257,   922,\n",
       "         1295,   284,   923,   764])>,\n",
       " <tf.Tensor: shape=(22, 1), dtype=int64, numpy=\n",
       " array([[ 0],\n",
       "        [ 1],\n",
       "        [ 2],\n",
       "        [ 3],\n",
       "        [ 4],\n",
       "        [ 5],\n",
       "        [ 6],\n",
       "        [ 7],\n",
       "        [ 8],\n",
       "        [ 9],\n",
       "        [10],\n",
       "        [11],\n",
       "        [12],\n",
       "        [13],\n",
       "        [14],\n",
       "        [15],\n",
       "        [16],\n",
       "        [17],\n",
       "        [18],\n",
       "        [19],\n",
       "        [20],\n",
       "        [21]])>,\n",
       " <tf.Tensor: shape=(1,), dtype=int64, numpy=array([22])>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['input_ids'].values, example['input_ids'].indices, example['input_ids'].dense_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ed07e959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(50,), dtype=int32, numpy=\n",
      "array([ 1169,  3881,   318, 23985,   284,   307,   262,  2310,   301,\n",
      "        4289,   338,   649,   366,   369,   272,   366,   290,   326,\n",
      "         339,   338,  1016,   284,   787,   257, 22870,   772,  3744,\n",
      "         621,   610,    77,   727,  5513,  5767,    89, 44028,   837,\n",
      "         474, 11025,    12,   565,  3885,  5719,  1801,  1326,   393,\n",
      "        2876,   574,   384, 13528,   764], dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(53,), dtype=int32, numpy=\n",
      "array([ 1169, 17177,  3481, 15962, 24659,   286,   366,   262, 15876,\n",
      "         286,   262, 13917,   366, 26298,   318,   523,  3236,   326,\n",
      "         257,  5721,   286,  2456,  2314, 22668,  6901,   763,    12,\n",
      "       16002,    14, 35248,   279,  2357, 14509,  1559,   338,  9902,\n",
      "        5761,   286,   474,   764,   374,   764,   374,   764,   284,\n",
      "          75,    74,  2013,   338,  3504,    12, 16442,   764],\n",
      "      dtype=int32)>}\n",
      "{'input_ids': <tf.Tensor: shape=(9,), dtype=int32, numpy=\n",
      "array([16803,   475,  1165,    12,    83,   538,   312,  3182, 16603],\n",
      "      dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(ds):\n",
    "    if i == 3: break\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "49b7b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = 16\n",
    "# train_mbs_per_replica = 2 # train_micro_batch_size_per_gpu in deepspeed\n",
    "tpu_size = 8  # jax.num_devices()\n",
    "cores_per_replica = 4  # mp=4, dp=8/4=2\n",
    "per_replica_batch = 1  # train_mbs_per_replica in run_clm_mp_xd.py, train_micro_batch_size_per_gpu in deepspeed\n",
    "train_batch_size = (gradient_accumulation_steps, per_replica_batch * tpu_size // cores_per_replica)\n",
    "max_len = 80  # max(len(s) for s in sequences) == 78\n",
    "# ds = ds.apply(tf.data.experimental.dense_to_ragged_batch(np.prod(self.bs), drop_remainder=True)) # mtj\n",
    "ds = ds.padded_batch(batch_size=np.prod(train_batch_size), padded_shapes={'input_ids': [max_len]},\n",
    "                     padding_values={'input_ids': 0}, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ff03981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(32, 80), dtype=int32, numpy=\n",
      "array([[ 1169,  3881,   318, ...,     0,     0,     0],\n",
      "       [ 1169, 17177,  3481, ...,     0,     0,     0],\n",
      "       [16803,   475,  1165, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  270,  5419,   326, ...,     0,     0,     0],\n",
      "       [ 5162,  4741,  2308, ...,     0,     0,     0],\n",
      "       [   64,  4958,   913, ...,     0,     0,     0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for batch in ds: print(batch); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c2cc8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.prefetch(10)  # mesh-transformer-jax\n",
    "# ds = ds.repeat()  # gpt-neo/inputs.py\n",
    "# map shard directly over ds won't work, getting AttributeError: 'Tensor' object has no attribute 'numpy'\n",
    "# because inside tf.function?, see e.g.:\n",
    "# 1) https://stackoverflow.com/questions/34097281/convert-a-tensor-to-numpy-array-in-tensorflow\n",
    "# 2) https://github.com/tensorflow/tensorflow/issues/27519\n",
    "# ds = ds.map(partial(shard, batch_size=train_batch_size), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# matthias-wright/flaxmodels/training/stylegan2/data_pipeline.py\n",
    "ds_iter = iter(ds)\n",
    "ds_iter = map(lambda x: shard(x, batch_size=train_batch_size), ds_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4ffdb061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': array([[[ 1169,  3881,   318, ...,     0,     0,     0],\n",
      "        [ 1169, 17177,  3481, ...,     0,     0,     0]],\n",
      "\n",
      "       [[16803,   475,  1165, ...,     0,     0,     0],\n",
      "        [  361,   345,  3360, ...,     0,     0,     0]],\n",
      "\n",
      "       [[24677,  3212,   355, ...,     0,     0,     0],\n",
      "        [ 1169,  2646,  3769, ...,     0,     0,     0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 2777,  1304,   805, ...,     0,     0,     0],\n",
      "        [  272,  7306,  2569, ...,     0,     0,     0]],\n",
      "\n",
      "       [[  265,   546,  6957, ...,     0,     0,     0],\n",
      "        [  270,  5419,   326, ...,     0,     0,     0]],\n",
      "\n",
      "       [[ 5162,  4741,  2308, ...,     0,     0,     0],\n",
      "        [   64,  4958,   913, ...,     0,     0,     0]]], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "for batch in ds_iter: print(batch); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a9cf7d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2, 80)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "@seqio.map_over_dataset\n",
    "def convert_datatype(ex): return {k: tf.cast(tf.sparse.to_dense(v, default_value=0), dtype=tf.int32) for k, v in ex.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5928490",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqio.TaskRegistry.add(\n",
    "    \"rotten_tomatoes_train_8530\",\n",
    "    seqio.TFExampleDataSource(split_to_filepattern={'train': 'gs://jax_projects/data/rotten_tomatoes_train_8530.tfrecords'},\n",
    "                             feature_description={\"input_ids\": tf.io.VarLenFeature(tf.int64)}),\n",
    "    preprocessors=[convert_datatype,],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
