{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee519737",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3bd8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "# import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental.maps import thread_resources\n",
    "from jax.experimental.maps import Mesh\n",
    "from jax.experimental.maps import xmap\n",
    "from jax.experimental.pjit import pjit\n",
    "from jax.experimental import PartitionSpec as P\n",
    "\n",
    "import haiku as hk\n",
    "import optax\n",
    "\n",
    "from mesh_transformer import util\n",
    "from mesh_transformer.layers import EmbeddingShard, TransformerLayerShard, RelativePositionEmbs, ProjectionShard, \\\n",
    "    TransformerLayerShardV2, Projection, EmbeddingShardV2\n",
    "from mesh_transformer.checkpoint import read_ckpt, write_ckpt, write_ckpt_v2, load_ckpt_v2\n",
    "from mesh_transformer.transformer_shard import CausalTransformerShard, CausalTransformer, CausalTransformerV2, CausalTransformerV2\n",
    "from mesh_transformer.util import clip_by_global_norm, additive_weight_decay, to_f32, to_bf16, maybe_shard, head_print, global_norm\n",
    "# from tfrecord_loader import TFRecordNewInputs\n",
    "# from smart_open import open\n",
    "# from google.cloud import storage\n",
    "# from google.cloud.exceptions import NotFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc729ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "from flax.training import train_state\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.core.frozen_dict import freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac88a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax devices: 4\n"
     ]
    }
   ],
   "source": [
    "tpu_size = jax.device_count(); print(f\"jax devices: {tpu_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffb50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_per_host = cores_per_replica = 2\n",
    "mesh_shape = (tpu_size // cores_per_replica, cores_per_replica)\n",
    "devices = np.array(jax.devices()).reshape(mesh_shape)\n",
    "cpu_device = jax.devices('cpu')[0]\n",
    "mesh = Mesh(devices, ('dp', 'mp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_shape = (4, 2)  # assume we hav 8 devices\n",
    "devices = np.asarray(jax.devices()).reshape(*mesh_shape)\n",
    "mesh = Mesh(devices, ('x', 'y'))\n",
    "mesh = Mesh(devices, ('mp', 'dp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b8c9930a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 2,  3],\n",
       "       [ 4,  5],\n",
       "       [ 6,  7],\n",
       "       [ 8,  9],\n",
       "       [10, 11],\n",
       "       [12, 13],\n",
       "       [14, 15]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[56., 64.],\n",
       "       [56., 64.],\n",
       "       [56., 64.],\n",
       "       [56., 64.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.arange(8 * 2).reshape(8, 2); w  # io\n",
    "x = np.ones((4, 8)); x  # bi\n",
    "def forward(x, w): return x @ w  # bi,io->bo\n",
    "forward(x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a674fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_pjit = pjit(lambda x: x, in_axis_resources=None, out_axis_resources=P('mp', None))\n",
    "with mesh: w = init_pjit(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d5b1257",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_pjit = pjit(forward, in_axis_resources=(P('dp', 'mp'), P('mp', None)), out_axis_resources=None)\n",
    "with mesh: y = forward_pjit(x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86377c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11, 12, 13, 14, 15]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.arange(2 * 8).reshape(2, 8); w  # io\n",
    "x = np.ones((4, 2)); x  # bi\n",
    "forward(x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73951ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel = ('mp', 'dp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ccf4c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_pjit = pjit(lambda x: x, in_axis_resources=None, out_axis_resources=P(None, parallel))\n",
    "with mesh: w = init_pjit(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79c323c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_pjit = pjit(forward, in_axis_resources=(P('dp', None), P(None, parallel)),\n",
    "                             out_axis_resources=P('dp', 'mp'))\n",
    "with mesh: y = forward_pjit(x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae18b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98052375",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_pjit = pjit(lambda x: x, in_axis_resources=None, out_axis_resources=P(parallel, None))\n",
    "with mesh: w = init_pjit(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "72cea6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_pjit = pjit(forward, in_axis_resources=(P('dp', 'mp'), P(parallel, None)), out_axis_resources=None)\n",
    "with mesh: y = forward_pjit(x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa91bfbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9101592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable\n",
    "from flax import core, struct\n",
    "\n",
    "class TrainState(struct.PyTreeNode):\n",
    "    step: int\n",
    "    apply_fn: Callable = struct.field(pytree_node=False)\n",
    "    params: core.FrozenDict[str, Any]\n",
    "    tx: optax.GradientTransformation = struct.field(pytree_node=False)\n",
    "    opt_state: optax.OptState\n",
    "\n",
    "    def apply_gradients(self, *, grads, **kwargs):\n",
    "        updates, new_opt_state = self.tx.update(grads, self.opt_state, self.params)\n",
    "        new_params = optax.apply_updates(self.params, to_f32(updates))  # XD: to_f32 from mesh-transformer-jax\n",
    "        return self.replace(step=self.step + 1, params=new_params, opt_state=new_opt_state, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, *, apply_fn, params, tx, **kwargs):\n",
    "        opt_state = tx.init(params)\n",
    "        return cls(step=0, apply_fn=apply_fn, params=params, tx=tx, opt_state=opt_state, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c9790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = params = json.load(open('configs/example_config.json'))\n",
    "per_replica_batch = params[\"per_replica_batch\"]\n",
    "cores_per_replica = params[\"cores_per_replica\"]\n",
    "\n",
    "scheduler = util.gpt3_schedule(config[\"warmup_steps\"], config[\"anneal_steps\"], config[\"lr\"], config[\"end_lr\"])\n",
    "optimizer = optax.chain(\n",
    "    optax.scale(1 / config.get(\"gradient_accumulation_steps\", 1)),\n",
    "    clip_by_global_norm(1, use_psum=False),\n",
    "    optax.scale_by_adam(),\n",
    "    additive_weight_decay(config[\"weight_decay\"]),\n",
    "    optax.scale(-1),\n",
    "    optax.scale_by_schedule(scheduler)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efecdf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = hk.PRNGSequence(42)  # PRNGKey(42)\n",
    "seq, vocab = config[\"seq\"], config[\"n_vocab\"]\n",
    "example_shape, train_example_shape = (1, seq,), (1, 1, seq)\n",
    "x = jax.random.uniform(next(key), example_shape, minval=0, maxval=vocab).astype(jnp.uint32)  # batch, len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b741364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(key, x):\n",
    "    def train_loss(x, y): return CausalTransformerShard(config).loss(x, y)\n",
    "    param_init_fn = hk.transform(hk.experimental.optimize_rng_use(train_loss)).init\n",
    "    params = param_init_fn(key, x, x)\n",
    "    return params\n",
    "\n",
    "params = init_params(next(key), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dd562923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loss(batch):\n",
    "    transformer = CausalTransformerShard(config)\n",
    "    out = transformer.loss(**batch, z_loss=True)\n",
    "    return out[\"loss\"], out[\"last_loss\"]\n",
    "loss_apply_fn = hk.without_apply_rng(hk.transform(train_loss)).apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6e862dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(params): return params, optimizer.init(params)\n",
    "state_shapes = jax.eval_shape(init, params)\n",
    "params_spec = jax.tree_map(partial(shard_strategy, parallel=(\"mp\", \"dp\")), state_shapes[0])\n",
    "params_spec = freeze(params_spec)\n",
    "# params = freeze(params)\n",
    "\n",
    "def get_opt_spec(x): return params_spec if isinstance(x, dict) else None  # from run_clm_mp.py\n",
    "params_spec, opt_state_spec = jax.tree_map(get_opt_spec, state_shapes, is_leaf=lambda x: isinstance(x, (dict, optax.EmptyState)))\n",
    "state_spec = TrainState(params=params_spec, opt_state=opt_state_spec, step=None, apply_fn=loss_apply_fn, tx=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "49f29d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dalle-mini\n",
    "params = jax.tree_map(lambda x: np.asarray(x), params)  # from run_clm_mp.py\n",
    "def init_state(params): return TrainState.create(apply_fn=loss_apply_fn, tx=optimizer, params=params)\n",
    "# with mesh: params, opt_state = pjit(init_state, None, state_shard)(params)\n",
    "with mesh: state = pjit(init_state, (params_spec,), state_spec, donate_argnums=(0,))(freeze(params))  # in_axis_resources should be None?\n",
    "# del params  # free CPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7761060c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    causal_transformer_shard/~/embedding_shard_v2/~/linear: {\n",
       "        b: [((32,), GpuDevice(id=0, process_index=0)), ((32,), GpuDevice(id=1, process_index=0)), ((32,), GpuDevice(id=2, process_index=0)), ((32,), GpuDevice(id=3, process_index=0)), ((32,), GpuDevice(id=4, process_index=0)), ((32,), GpuDevice(id=5, process_index=0)), ((32,), GpuDevice(id=6, process_index=0)), ((32,), GpuDevice(id=7, process_index=0))],\n",
       "        w: [((8, 32), GpuDevice(id=0, process_index=0)), ((8, 32), GpuDevice(id=1, process_index=0)), ((8, 32), GpuDevice(id=2, process_index=0)), ((8, 32), GpuDevice(id=3, process_index=0)), ((8, 32), GpuDevice(id=4, process_index=0)), ((8, 32), GpuDevice(id=5, process_index=0)), ((8, 32), GpuDevice(id=6, process_index=0)), ((8, 32), GpuDevice(id=7, process_index=0))],\n",
       "    },\n",
       "    causal_transformer_shard/~/layer_0/~/fc_in: {\n",
       "        b: [((16,), GpuDevice(id=0, process_index=0)), ((16,), GpuDevice(id=1, process_index=0)), ((16,), GpuDevice(id=2, process_index=0)), ((16,), GpuDevice(id=3, process_index=0)), ((16,), GpuDevice(id=4, process_index=0)), ((16,), GpuDevice(id=5, process_index=0)), ((16,), GpuDevice(id=6, process_index=0)), ((16,), GpuDevice(id=7, process_index=0))],\n",
       "        w: [((32, 16), GpuDevice(id=0, process_index=0)), ((32, 16), GpuDevice(id=1, process_index=0)), ((32, 16), GpuDevice(id=2, process_index=0)), ((32, 16), GpuDevice(id=3, process_index=0)), ((32, 16), GpuDevice(id=4, process_index=0)), ((32, 16), GpuDevice(id=5, process_index=0)), ((32, 16), GpuDevice(id=6, process_index=0)), ((32, 16), GpuDevice(id=7, process_index=0))],\n",
       "    },\n",
       "    causal_transformer_shard/~/layer_0/~/fc_out: {\n",
       "        b: [((32,), GpuDevice(id=0, process_index=0)), ((32,), GpuDevice(id=1, process_index=0)), ((32,), GpuDevice(id=2, process_index=0)), ((32,), GpuDevice(id=3, process_index=0)), ((32,), GpuDevice(id=4, process_index=0)), ((32,), GpuDevice(id=5, process_index=0)), ((32,), GpuDevice(id=6, process_index=0)), ((32,), GpuDevice(id=7, process_index=0))],\n",
       "        w: [((16, 32), GpuDevice(id=0, process_index=0)), ((16, 32), GpuDevice(id=1, process_index=0)), ((16, 32), GpuDevice(id=2, process_index=0)), ((16, 32), GpuDevice(id=3, process_index=0)), ((16, 32), GpuDevice(id=4, process_index=0)), ((16, 32), GpuDevice(id=5, process_index=0)), ((16, 32), GpuDevice(id=6, process_index=0)), ((16, 32), GpuDevice(id=7, process_index=0))],\n",
       "    },\n",
       "    causal_transformer_shard/~/layer_0/~/layer_norm: {\n",
       "        offset: [((32,), GpuDevice(id=0, process_index=0)), ((32,), GpuDevice(id=1, process_index=0)), ((32,), GpuDevice(id=2, process_index=0)), ((32,), GpuDevice(id=3, process_index=0)), ((32,), GpuDevice(id=4, process_index=0)), ((32,), GpuDevice(id=5, process_index=0)), ((32,), GpuDevice(id=6, process_index=0)), ((32,), GpuDevice(id=7, process_index=0))],\n",
       "        scale: [((32,), GpuDevice(id=0, process_index=0)), ((32,), GpuDevice(id=1, process_index=0)), ((32,), GpuDevice(id=2, process_index=0)), ((32,), GpuDevice(id=3, process_index=0)), ((32,), GpuDevice(id=4, process_index=0)), ((32,), GpuDevice(id=5, process_index=0)), ((32,), GpuDevice(id=6, process_index=0)), ((32,), GpuDevice(id=7, process_index=0))],\n",
       "    },\n",
       "    causal_transformer_shard/~/layer_0/~/o_proj: {\n",
       "        w: [((4, 32), GpuDevice(id=0, process_index=0)), ((4, 32), GpuDevice(id=1, process_index=0)), ((4, 32), GpuDevice(id=2, process_index=0)), ((4, 32), GpuDevice(id=3, process_index=0)), ((4, 32), GpuDevice(id=4, process_index=0)), ((4, 32), GpuDevice(id=5, process_index=0)), ((4, 32), GpuDevice(id=6, process_index=0)), ((4, 32), GpuDevice(id=7, process_index=0))],\n",
       "    },\n",
       "    causal_transformer_shard/~/layer_0/~/qkv_proj: {\n",
       "        w: [((32, 12), GpuDevice(id=0, process_index=0)), ((32, 12), GpuDevice(id=1, process_index=0)), ((32, 12), GpuDevice(id=2, process_index=0)), ((32, 12), GpuDevice(id=3, process_index=0)), ((32, 12), GpuDevice(id=4, process_index=0)), ((32, 12), GpuDevice(id=5, process_index=0)), ((32, 12), GpuDevice(id=6, process_index=0)), ((32, 12), GpuDevice(id=7, process_index=0))],\n",
       "    },\n",
       "    causal_transformer_shard/~/projection/~/layer_norm: {\n",
       "        offset: [((32,), GpuDevice(id=0, process_index=0)), ((32,), GpuDevice(id=1, process_index=0)), ((32,), GpuDevice(id=2, process_index=0)), ((32,), GpuDevice(id=3, process_index=0)), ((32,), GpuDevice(id=4, process_index=0)), ((32,), GpuDevice(id=5, process_index=0)), ((32,), GpuDevice(id=6, process_index=0)), ((32,), GpuDevice(id=7, process_index=0))],\n",
       "        scale: [((32,), GpuDevice(id=0, process_index=0)), ((32,), GpuDevice(id=1, process_index=0)), ((32,), GpuDevice(id=2, process_index=0)), ((32,), GpuDevice(id=3, process_index=0)), ((32,), GpuDevice(id=4, process_index=0)), ((32,), GpuDevice(id=5, process_index=0)), ((32,), GpuDevice(id=6, process_index=0)), ((32,), GpuDevice(id=7, process_index=0))],\n",
       "    },\n",
       "    causal_transformer_shard/~/projection/~/linear: {\n",
       "        b: [((8,), GpuDevice(id=0, process_index=0)), ((8,), GpuDevice(id=1, process_index=0)), ((8,), GpuDevice(id=2, process_index=0)), ((8,), GpuDevice(id=3, process_index=0)), ((8,), GpuDevice(id=4, process_index=0)), ((8,), GpuDevice(id=5, process_index=0)), ((8,), GpuDevice(id=6, process_index=0)), ((8,), GpuDevice(id=7, process_index=0))],\n",
       "        w: [((32, 8), GpuDevice(id=0, process_index=0)), ((32, 8), GpuDevice(id=1, process_index=0)), ((32, 8), GpuDevice(id=2, process_index=0)), ((32, 8), GpuDevice(id=3, process_index=0)), ((32, 8), GpuDevice(id=4, process_index=0)), ((32, 8), GpuDevice(id=5, process_index=0)), ((32, 8), GpuDevice(id=6, process_index=0)), ((32, 8), GpuDevice(id=7, process_index=0))],\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_map(lambda x: [(db.shape, db.device()) for db in x.device_buffers], state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e659faa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xd/.local/lib/python3.7/site-packages/jax/experimental/maps.py:459: UserWarning: xmap is an experimental feature and probably has bugs!\n",
      "  warn(\"xmap is an experimental feature and probably has bugs!\")\n"
     ]
    }
   ],
   "source": [
    "init_xmap = xmap(fun=init,\n",
    "                in_axes=([\"shard\", ...], [\"batch\", ...]),\n",
    "                out_axes=[\"shard\", ...],\n",
    "                axis_resources={'shard': 'mp', 'batch': 'dp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9e64337",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mesh: state = init_xmap(jnp.array(key.take(mp_per_host)), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef436ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard_strategy(shape_dtype, parallel):\n",
    "    if shape_dtype.ndim == 0:\n",
    "        return P()\n",
    "    if shape_dtype.ndim == 1:\n",
    "        if shape_dtype.shape[0] == config[\"d_model\"]: # layernorm or fc_out bias\n",
    "            return P(None)\n",
    "        elif shape_dtype.shape[0] == config[\"n_vocab\"]:  # projection bias\n",
    "            return P(parallel)\n",
    "        else:\n",
    "            assert shape_dtype.shape[0] == config[\"d_model\"] * 4, str(shape_dtype)  # fc_in bias\n",
    "            return P(parallel)\n",
    "    assert shape_dtype.ndim == 2, str(shape_dtype)\n",
    "    # embedding/projection layers\n",
    "    if shape_dtype.shape == (config[\"n_vocab\"], config[\"d_model\"]):\n",
    "        return P(parallel, None)\n",
    "    elif shape_dtype.shape == (config[\"d_model\"], config[\"n_vocab\"]):\n",
    "        return P(None, parallel)\n",
    "\n",
    "    # a transformer layer\n",
    "    elif shape_dtype.shape[0] == config[\"d_model\"] or shape_dtype.shape[1] == config[\"d_model\"]:\n",
    "        # shard along the axis which is _not_ the model dimension\n",
    "        if shape_dtype.shape[1] == config[\"d_model\"]:\n",
    "            return P(parallel, None)\n",
    "        elif shape_dtype.shape[0] == config[\"d_model\"]:\n",
    "            return P(None, parallel)\n",
    "    else:\n",
    "        raise NotImplementedError(\"borked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9beb4657",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a72095bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_step(state, ctx, tgt, ctx_length):\n",
    "def eval_step(params, batch):\n",
    "    ctx, tgt, ctx_length = batch\n",
    "    def eval_loss(x, y, mask):\n",
    "        transformer = CausalTransformerShard(config)\n",
    "        return transformer.loss(x, y, mask=mask)#['loss']\n",
    "    eval_loss_fn = hk.without_apply_rng(hk.transform(eval_loss)).apply\n",
    "\n",
    "    # mask = (jnp.arange(0, len(ctx)) > ctx_length) * -1e10  # XD: j\n",
    "    # XD: copied from V2\n",
    "    # mask = (jnp.arange(0, ctx.shape[1])[None, :] > ctx_length[:, None]) * -1e10  # XD: bj\n",
    "    # mask = mask[:, None, None, :]  # XD: bj->bnij\n",
    "#     return eval_loss_fn(to_bf16(state[\"params\"]), ctx, tgt, mask)\n",
    "    return eval_loss_fn(to_bf16(params), ctx, tgt, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9a464e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(state, batch):  # ctx, tgt, ctx_length\n",
    "    def eval_loss(batch):  # ctx, tgt, mask\n",
    "        return CausalTransformerShard(config).loss(**batch)#['loss']\n",
    "    eval_loss_fn = hk.without_apply_rng(hk.transform(eval_loss)).apply\n",
    "    # XD: copied from V2\n",
    "    # mask = (jnp.arange(0, ctx.shape[1])[None, :] > ctx_length[:, None]) * -1e10  # XD: bj\n",
    "    # mask = mask[:, None, None, :]  # XD: bj->bnij\n",
    "    mask = 0.\n",
    "    batch = {k: v for k, v in batch.items() if k != 'ctx_length'}\n",
    "    batch['mask'] = mask\n",
    "    return eval_loss_fn(to_bf16(state.params), batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c3f14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_step(state, ctx, tgt):\n",
    "#     params, opt_state = state['params'], state['opt_state']\n",
    "def train_step(state, step, batch):\n",
    "    ctx, tgt = batch\n",
    "    params, opt_state = state\n",
    "    def train_loss(x, y):\n",
    "        transformer = CausalTransformerShard(config)\n",
    "        out = transformer.loss(x, y, z_loss=True)\n",
    "        return out[\"loss\"], out[\"last_loss\"]\n",
    "    train_loss_fn = hk.without_apply_rng(hk.transform(train_loss)).apply\n",
    "\n",
    "    def microbatch(old_grad, batch):\n",
    "        ctx, tgt = batch\n",
    "        val_grad_fn = jax.value_and_grad(train_loss_fn, has_aux=True)\n",
    "        (loss, last_loss), grad = val_grad_fn(to_bf16(params), ctx, tgt)\n",
    "\n",
    "        new_grad = jax.tree_multimap(lambda a, b: a + b, old_grad, grad)\n",
    "        # gnorm = global_norm(grad)\n",
    "        return new_grad, (loss, last_loss)#, gnorm)\n",
    "\n",
    "    if ctx.shape[0] == 1:\n",
    "        val_grad_fn = jax.value_and_grad(train_loss_fn, has_aux=True)\n",
    "        (loss, last_loss), grad = val_grad_fn(to_bf16(state.params), ctx[0], tgt[0])\n",
    "    else:\n",
    "        grad, (loss, last_loss, gnorm) = jax.lax.scan(microbatch,\n",
    "                                               jax.tree_map(lambda x: jnp.zeros_like(x).astype(jnp.bfloat16), params),\n",
    "                                               (ctx, tgt))\n",
    "    grad = jax.lax.pmean(grad, \"batch\")  # for xmap,  # XDC: loss and last_loss are not pmeaned accross batch dim\n",
    "    updates, new_opt_state = optimizer.update(grad, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, to_f32(updates))\n",
    "    return to_f32(loss), to_f32(last_loss), (new_params, tuple(new_opt_state)), step + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b5cde9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, batch):\n",
    "    params, train_loss_fn = state.params, state.apply_fn\n",
    "\n",
    "    def microbatch(old_grad, batch):\n",
    "        val_grad_fn = jax.value_and_grad(train_loss_fn, has_aux=True)\n",
    "        (loss, last_loss), grad = val_grad_fn(to_bf16(params), batch)\n",
    "        new_grad = jax.tree_multimap(lambda a, b: a + b, old_grad, grad)\n",
    "        # gnorm = global_norm(grad)\n",
    "        return new_grad, (loss, last_loss)#, gnorm)\n",
    "\n",
    "    if batch['ctx'].shape[0] == 1:\n",
    "        val_grad_fn = jax.value_and_grad(train_loss_fn, has_aux=True)\n",
    "        (loss, last_loss), grad = val_grad_fn(to_bf16(params), jax.tree_map(lambda x: x[0], batch))\n",
    "    else:\n",
    "        grad, (loss, last_loss, gnorm) = jax.lax.scan(microbatch,\n",
    "                                               jax.tree_map(lambda x: jnp.zeros_like(x).astype(jnp.bfloat16), params),\n",
    "                                               batch)\n",
    "#     grad = jax.lax.pmean(grad, \"batch\")  # for xmap,  # XDC: loss and last_loss are not pmeaned accross batch dim\n",
    "    state = state.apply_gradients(grads=grad)\n",
    "    metrics = {'loss': to_f32(loss), 'last_loss': to_f32(last_loss)}\n",
    "    return state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a1c0a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {'obs': x[:, :-1], 'target': x[:, 1:]}\n",
    "ctx_length = jnp.array([len(sample[\"obs\"][0])] * len(sample[\"obs\"]))\n",
    "batch = {'ctx': x[:, :-1], 'tgt': x[:, 1:], 'ctx_length': ctx_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "45e0823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss0 = eval_step(params, (sample['obs'], sample['target'], ctx_length))\n",
    "eval_metrics = eval_step(state, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f26af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_eval_step = pjit(eval_step, (state_shard[0], (P('dp'), P('dp'), P('dp'))), None)\n",
    "with mesh: loss = p_eval_step(params, (sample['obs'], sample['target'], ctx_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a0f7c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_eval_step = pjit(eval_step, (state_spec, P('dp')), None)\n",
    "with mesh: eval_metrics = p_eval_step(state, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6176efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xd/.local/lib/python3.7/site-packages/jax/experimental/maps.py:459: UserWarning: xmap is an experimental feature and probably has bugs!\n",
      "  warn(\"xmap is an experimental feature and probably has bugs!\")\n"
     ]
    }
   ],
   "source": [
    "eval_step_xmap = xmap(fun=eval_step,\n",
    "                in_axes=([\"shard\", ...], ([\"batch\", ...], [\"batch\", ...], [\"batch\", ...])),\n",
    "                out_axes=[\"batch\", ...],\n",
    "                axis_resources={'shard': 'mp', 'batch': 'dp'})\n",
    "with mesh: outputs = eval_step_xmap(state[0], (sample['obs'], sample['target'], ctx_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b75da81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {'obs': x[:, :-1][None, ...], 'target': x[:, 1:][None, ...]}\n",
    "step = np.ones((mp_per_host,)).astype('int32')\n",
    "batch = {'ctx': x[:, :-1][None, ...], 'tgt': x[:, 1:][None, ...]}\n",
    "# batch = freeze(batch)  # from dalle-mini freeze batch to pass safely to jax transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5440ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = (params, opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c723fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/shawn_guo/miniconda3/envs/torch1.7/lib/python3.7/site-packages/jax/_src/tree_util.py:201: FutureWarning: jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() instead as a drop-in replacement.\n",
      "  'instead as a drop-in replacement.', FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "state, train_metrics = train_step(state, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "567d1a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': DeviceArray(4.151296, dtype=float32),\n",
       " 'last_loss': DeviceArray(4.151296, dtype=float32)}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04a036fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train_step = pjit(train_step, (state_shard, None, (P(None, 'dp'), P(None, 'dp'))),\n",
    "                                (None, None, state_shard, None))\n",
    "with mesh: loss, last_loss, state, step = p_train_step(state, step, (sample['obs'], sample['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1a58b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train_step = pjit(train_step, (state_spec, P(None, 'dp')), (state_spec, None))\n",
    "with mesh: state, train_metrics = p_train_step(state, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc97074e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xd/.local/lib/python3.7/site-packages/jax/experimental/maps.py:459: UserWarning: xmap is an experimental feature and probably has bugs!\n",
      "  warn(\"xmap is an experimental feature and probably has bugs!\")\n",
      "/home/xd/.local/lib/python3.7/site-packages/jax/_src/tree_util.py:201: FutureWarning: jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() instead as a drop-in replacement.\n",
      "  'instead as a drop-in replacement.', FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_step_xmap = xmap(fun=train_step,\n",
    "                 in_axes=([\"shard\", ...], [\"shard\", ...], ([\"batch\", ...], [\"batch\", ...])),\n",
    "                 out_axes=([\"batch\", ...], [\"batch\", ...], [\"shard\", ...], [\"shard\", ...]),\n",
    "                 donate_argnums=(0,),  # also needed by pjit\n",
    "                 axis_resources={'shard': 'mp', 'batch': 'dp'})\n",
    "with mesh: loss, last_loss, state, step = train_step_xmap(state, step, (sample['obs'], sample['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a2eae70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ShardedDeviceArray([4.2005105], dtype=float32),\n",
       " ShardedDeviceArray([3.0642111], dtype=float32),\n",
       " ShardedDeviceArray([2, 2, 2, 2, 2, 2, 2, 2], dtype=int32))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, last_loss, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca261770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loss(x, y):\n",
    "    transformer = CausalTransformerShard(config)\n",
    "    return transformer.loss(x, y)\n",
    "\n",
    "net = hk.without_apply_rng(hk.transform(train_loss))\n",
    "params = net.init(jax.random.PRNGKey(42), x, x)\n",
    "param_shapes = jax.eval_shape(init, jax.random.PRNGKey(42), x)\n",
    "net.apply(params, x[:, :-1], x[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1940c5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xd/.local/lib/python3.7/site-packages/jax/experimental/maps.py:412: UserWarning: xmap is an experimental feature and probably has bugs!\n",
      "  warn(\"xmap is an experimental feature and probably has bugs!\")\n"
     ]
    }
   ],
   "source": [
    "with jax.experimental.maps.mesh(devices, ('dp', 'mp')): \n",
    "    init_xmap = jax.experimental.maps.xmap(fun=init,\n",
    "                                        in_axes=([\"shard\", ...],\n",
    "                                                 [\"batch\", ...]),\n",
    "                                        out_axes=[\"shard\", ...],\n",
    "                                        axis_resources={'shard': 'mp', 'batch': 'dp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5119fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jax.experimental.maps.mesh(devices, ('dp', 'mp')):\n",
    "    out = self.eval_xmap(self.state, sample[\"obs\"], sample[\"target\"], ctx_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
